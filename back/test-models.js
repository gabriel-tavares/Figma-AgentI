/**
 * test-models.js
 * Configura e testa diferentes modelos OpenAI para an√°lise heur√≠stica.
 * 
 * Uso:
 *   node test-models.js o3-mini     # configura e mostra como rodar
 *   node test-models.js gpt-4o       # outro modelo
 *   node test-models.js list         # lista todos dispon√≠veis
 * 
 * Depois de configurar, rode: node index.js
 */

const fs = require('fs');
const path = require('path');

// Configura√ß√µes globais de tokens
const TOKENS_CONFIG = {
  DEFAULT: 8000,      // Tokens padr√£o para modelos regulares
  PREMIUM: 12000,      // Tokens para GPT-5 (maior qualidade)
  O3: 15000           // Tokens para modelos O3 (limite TPM menor)
};

// Configura√ß√µes globais de temperatura
const TEMP_CONFIG = {
  DEFAULT: 0.1,        // Temperatura padr√£o para modelos regulares
  PREMIUM: 0.1,        // Temperatura para GPT-5 (maior criatividade)
  O3: null,            // O3 n√£o usa temperatura
  O4: null             // O4 n√£o usa temperatura
};

// Configura√ß√µes globais de reasoning (apenas para modelos O3/O4)
const REASONING_CONFIG = {
  O3_MINI: 'medium',   // Reasoning para o3-mini
  O3: 'high',         // Reasoning para o3
  O4_MINI: 'high'     // Reasoning para o4-mini (pr√≥xima gera√ß√£o)
};

// Configura√ß√µes dos modelos dispon√≠veis (apenas etapa textual)
const MODEL_CONFIGS = {
  'gpt-4.1-mini': {
    texto: { model: 'gpt-4.1-mini', temp: TEMP_CONFIG.DEFAULT, tokens: TOKENS_CONFIG.DEFAULT },
    description: 'Balanceado - bom custo-benef√≠cio'
  },
  'gpt-4o-mini': {
    texto: { model: 'gpt-4o-mini', temp: TEMP_CONFIG.DEFAULT, tokens: TOKENS_CONFIG.DEFAULT },
    description: 'Vision otimizada - melhor para imagens'
  },
  'gpt-4o': {
    texto: { model: 'gpt-4o', temp: TEMP_CONFIG.DEFAULT, tokens: TOKENS_CONFIG.DEFAULT },
    description: 'Alta qualidade - mais preciso'
  },
  'gpt-5': {
    texto: { model: 'gpt-5', temp: TEMP_CONFIG.PREMIUM, tokens: TOKENS_CONFIG.PREMIUM },
    description: 'Premium - m√°xima qualidade textual'
  },
  'o3-mini': {
    texto: { model: 'o3-mini', tokens: TOKENS_CONFIG.O3, reasoning: REASONING_CONFIG.O3_MINI },
    description: 'O3 determin√≠stico - an√°lise mais profunda'
  },
  'o3': {
    texto: { model: 'o3', tokens: TOKENS_CONFIG.O3, reasoning: REASONING_CONFIG.O3 },
    description: 'O3 m√°ximo - an√°lise mais complexa'
  },
  'o4-mini': {
    texto: { model: 'o4-mini', temp: TEMP_CONFIG.O4, tokens: TOKENS_CONFIG.O3, reasoning: REASONING_CONFIG.O4_MINI },
    description: 'O4-mini - pr√≥xima gera√ß√£o com an√°lise avan√ßada'
  }
};

function listModels() {
  console.log('üìã Modelos dispon√≠veis:\n');
  Object.entries(MODEL_CONFIGS).forEach(([name, config]) => {
    console.log(`üîπ ${name}`);
    if (config.texto.temp) {
      console.log(`   Texto: ${config.texto.model} (temp: ${config.texto.temp})`);
    } else {
      console.log(`   Texto: ${config.texto.model}`);
    }
    if (config.texto.reasoning) {
      console.log(`   Reasoning: ${config.texto.reasoning}`);
    }
    console.log(`   ${config.description}\n`);
  });
}

function generateEnvFile(modelName) {
  const config = MODEL_CONFIGS[modelName];
  if (!config) {
    console.error(`‚ùå Modelo "${modelName}" n√£o encontrado.`);
    console.log('Use "node test-models.js list" para ver modelos dispon√≠veis.');
    return false;
  }

  // Ler .env existente para preservar configura√ß√µes importantes
  let existingEnv = {};
  const envPath = path.join(__dirname, '.env');
  try {
    const envContent = fs.readFileSync(envPath, 'utf8');
    envContent.split('\n').forEach(line => {
      const [key, ...valueParts] = line.split('=');
      if (key && valueParts.length > 0) {
        const keyTrim = key.trim();
        const valueTrim = valueParts.join('=').trim();
        // Preservar apenas configura√ß√µes importantes que n√£o s√£o padr√£o
        if (keyTrim === 'OPENAI_API_KEY' && !valueTrim.includes('your-key-here')) {
          existingEnv[keyTrim] = valueTrim;
        }
        if (keyTrim === 'VECTOR_STORE_ID' && !valueTrim.includes('your_vector_store_id')) {
          existingEnv[keyTrim] = valueTrim;
        }
      }
    });
  } catch (e) {
    // Arquivo .env n√£o existe, usar valores padr√£o
  }

  const envContent = `# Configura√ß√£o para ${modelName}
# Gerado automaticamente em ${new Date().toISOString()}

# Chave da API
OPENAI_API_KEY=${existingEnv.OPENAI_API_KEY || 'sk-your-key-here'}

# Modelos
MODELO_TEXTO=${config.texto.model}
MODELO_VISION=gpt-4.1-mini

# Temperaturas
${config.texto.temp ? `TEMP_TEXTO=${config.texto.temp}` : '# TEMP_TEXTO=0.1 (ignorado para modelos O3)'}
TEMP_VISION=0.1

# Tokens m√°ximos
MAXTOK_TEXTO=${config.texto.tokens}
MAXTOK_VISION=20000

# RAG
USE_RAG=true
VECTOR_STORE_ID=${existingEnv.VECTOR_STORE_ID || 'vs_your_vector_store_id'}

# API
USE_RESPONSES=true

# Servidor
PORT=3000

# Reasoning (apenas para modelos O3)
${config.texto.reasoning ? `REASONING_EFFORT=${config.texto.reasoning}` : '# REASONING_EFFORT=medium'}
`;

  return envContent;
}

function updateEnvFile(modelName) {
  const config = MODEL_CONFIGS[modelName];
  if (!config) {
    console.error(`‚ùå Modelo "${modelName}" n√£o encontrado.`);
    return false;
  }

  const envPath = path.join(__dirname, '.env');
  
  try {
    // Ler configura√ß√µes existentes importantes
    let existingEnv = {};
    try {
      const envContent = fs.readFileSync(envPath, 'utf8');
      envContent.split('\n').forEach(line => {
        const [key, ...valueParts] = line.split('=');
        if (key && valueParts.length > 0) {
          const keyTrim = key.trim();
          const valueTrim = valueParts.join('=').trim();
          // Preservar apenas configura√ß√µes importantes que n√£o s√£o padr√£o
          if (keyTrim === 'OPENAI_API_KEY' && !valueTrim.includes('your-key-here')) {
            existingEnv[keyTrim] = valueTrim;
          }
          if (keyTrim === 'VECTOR_STORE_ID' && !valueTrim.includes('your_vector_store_id')) {
            existingEnv[keyTrim] = valueTrim;
          }
        }
      });
    } catch (e) {
      // Arquivo .env n√£o existe, usar valores padr√£o
    }

    // Gerar arquivo .env limpo
    const envContent = `# Configura√ß√£o para ${modelName}
# Gerado automaticamente em ${new Date().toISOString()}

# Chave da API
OPENAI_API_KEY=${existingEnv.OPENAI_API_KEY || 'sk-your-key-here'}

# Modelos
MODELO_TEXTO=${config.texto.model}
MODELO_VISION=gpt-4.1-mini

# Temperaturas
${config.texto.temp ? `TEMP_TEXTO=${config.texto.temp}` : '# TEMP_TEXTO=0.1 (ignorado para modelos O3/O4)'}
TEMP_VISION=0.1

# Tokens m√°ximos
MAXTOK_TEXTO=${config.texto.tokens}
MAXTOK_VISION=20000

# RAG
USE_RAG=true
VECTOR_STORE_ID=${existingEnv.VECTOR_STORE_ID || 'vs_your_vector_store_id'}

# API
USE_RESPONSES=true

# Servidor
PORT=3000

# Reasoning (apenas para modelos O3/O4)
${config.texto.reasoning ? `REASONING_EFFORT=${config.texto.reasoning}` : '# REASONING_EFFORT=medium'}
`;

    // Escrever arquivo limpo
    fs.writeFileSync(envPath, envContent, 'utf8');
    console.log(`‚úÖ Arquivo .env atualizado para modelo: ${modelName}`);
    return true;
  } catch (error) {
    console.error(`‚ùå Erro ao atualizar .env:`, error.message);
    return false;
  }
}

function showConfig(modelName) {
  const config = MODEL_CONFIGS[modelName];
  if (!config) {
    console.error(`‚ùå Modelo "${modelName}" n√£o encontrado.`);
    return false;
  }

  console.log(`\nüîß Configura√ß√£o para: ${modelName}`);
  console.log(`üìù ${config.description}\n`);
  
  console.log('üìä Par√¢metros:');
  console.log(`   Vision: gpt-4.1-mini (fixo)`);
  console.log(`   Texto: ${config.texto.model}`);
  if (config.texto.temp) {
    console.log(`   - Temperatura: ${config.texto.temp} (${config.texto.temp === TEMP_CONFIG.DEFAULT ? 'padr√£o' : config.texto.temp === TEMP_CONFIG.PREMIUM ? 'premium' : 'custom'})`);
  }
  console.log(`   - Tokens: ${config.texto.tokens.toLocaleString()} (${config.texto.tokens === TOKENS_CONFIG.DEFAULT ? 'padr√£o' : config.texto.tokens === TOKENS_CONFIG.PREMIUM ? 'premium' : config.texto.tokens === TOKENS_CONFIG.O3 ? 'O3' : 'custom'})`);
  if (config.texto.reasoning) {
    let reasoningType = 'custom';
    if (modelName === 'o3-mini') reasoningType = 'O3-mini';
    else if (modelName === 'o3') reasoningType = 'O3';
    else if (modelName === 'o4-mini') reasoningType = 'O4-mini';
    
    console.log(`   - Reasoning: ${config.texto.reasoning} (${reasoningType})`);
  }
  
  console.log('');
  
  return true;
}

// Main execution
const args = process.argv.slice(2);
const command = args[0];

if (!command) {
  console.log('üîß Test Models - Configurador de Modelos OpenAI\n');
  console.log('Uso:');
  console.log('  node test-models.js <modelo>     # configura modelo espec√≠fico');
  console.log('  node test-models.js list        # lista modelos dispon√≠veis');
  console.log('  node test-models.js show <modelo> # mostra config sem aplicar\n');
  console.log('Exemplos:');
  console.log('  node test-models.js o3-mini');
  console.log('  node test-models.js gpt-4o');
  console.log('  node test-models.js list');
  process.exit(0);
}

if (command === 'list') {
  listModels();
  process.exit(0);
}

if (command === 'show') {
  const modelName = args[1];
  if (!modelName) {
    console.error('‚ùå Especifique um modelo: node test-models.js show <modelo>');
    process.exit(1);
  }
  showConfig(modelName);
  process.exit(0);
}

// Configurar modelo espec√≠fico
const modelName = command;
if (updateEnvFile(modelName)) {
  showConfig(modelName);
}
